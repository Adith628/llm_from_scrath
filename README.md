# ğŸ¤– Custom LLM from Scratch

A mini GPT implementation built from scratch using PyTorch, featuring both Shakespeare text generation and conversational AI capabilities.

## ğŸŒŸ Features

- **Character-level tokenization** with custom tokenizer
- **Transformer architecture** with multi-head attention
- **Two model versions**:
  - **V1**: Shakespeare text generation 
  - **V2**: Conversational AI chatbot
- **Interactive chat interface**
- **Configurable model architecture**
- **Training and evaluation scripts**

## ğŸ“ Project Structure

```
custom_llm/
â”œâ”€â”€ ğŸ“Š Data & Models
â”‚   â”œâ”€â”€ data/                    # Training data and tokenizers
â”‚   â”œâ”€â”€ checkpoints/             # Trained model weights
â”‚   â””â”€â”€ v1_shakespeare/          # V1 model backup
â”œâ”€â”€ ğŸ§  Core Components
â”‚   â”œâ”€â”€ model.py                 # GPT model architecture
â”‚   â”œâ”€â”€ tokenizer.py             # Character-level tokenizer
â”‚   â”œâ”€â”€ config.py                # Model and training configuration
â”‚   â””â”€â”€ prepare_dataset.py       # Dataset preparation
â”œâ”€â”€ ğŸš€ Training & Testing
â”‚   â”œâ”€â”€ train.py                 # Main training script
â”‚   â”œâ”€â”€ test_model.py            # Model testing utilities
â”‚   â””â”€â”€ quick_train_test.py      # Quick training validation
â”œâ”€â”€ ğŸ’¬ Chat Interface
â”‚   â”œâ”€â”€ chatbot_v2.py            # Interactive chatbot (V2)
â”‚   â””â”€â”€ simple_chat.py           # Simple chat interface
â”œâ”€â”€ ğŸ“‹ Utilities
â”‚   â”œâ”€â”€ download_chat_dataset.py # Conversation dataset creation
â”‚   â””â”€â”€ VERSION_HISTORY.md       # Version tracking
â””â”€â”€ ğŸ“š Documentation
    â”œâ”€â”€ README.md                # This file
    â””â”€â”€ requirements.txt         # Python dependencies
```

## ğŸ—ï¸ Model Architecture

```
Input Token IDs
â†“
Token Embeddings + Position Embeddings
â†“
[ Transformer Block Ã— N ]
  â”œâ”€â”€ Multi-Head Self-Attention
  â”œâ”€â”€ Layer Normalization
  â”œâ”€â”€ Feed Forward Network
  â””â”€â”€ Residual Connections
â†“
Final Layer Normalization
â†“
Linear Layer â†’ Vocabulary Size
â†“
Softmax (for probability distribution)
â†“
Predicted Next Token
```

### Architecture Specifications

| Component | V1 (Shakespeare) | V2 (Chat) |
|-----------|------------------|-----------|
| **Context Length** | 64 tokens | 128 tokens |
| **Embedding Dimension** | 64 | 128 |
| **Attention Heads** | 4 | 8 |
| **Transformer Layers** | 2 | 4 |
| **Parameters** | ~85K | ~680K |
| **Training Iterations** | 5,000 | 8,000 |

## ğŸš€ Quick Start

### Prerequisites

```bash
Python 3.8+
PyTorch 1.9+
```

### Installation

1. **Clone the repository**
```bash
git clone <repository-url>
cd custom_llm
```

2. **Create virtual environment**
```bash
python -m venv venv
venv\Scripts\activate  # Windows
# or
source venv/bin/activate  # Linux/Mac
```

3. **Install dependencies**
```bash
pip install -r requirements.txt
```

### Training a Model

#### Option 1: Train Shakespeare Model (V1)
```bash
# Use Shakespeare text (restore from backup if needed)
python prepare_dataset.py
python train.py
```

#### Option 2: Train Chat Model (V2)
```bash
# Create conversational dataset
python download_chat_dataset.py

# Prepare and train
python prepare_dataset.py
python train.py
```

### Testing the Model

#### Shakespeare Model (V1)
```bash
python test_model.py
```

#### Chat Model (V2)
```bash
# Interactive chat
python chatbot_v2.py

# Quick test
python chatbot_v2.py test
```

## ğŸ’» Usage Examples

### 1. Shakespeare Text Generation (V1)
```python
from test_model import load_trained_model, generate_text

model, tokenizer = load_trained_model()
text = generate_text(model, tokenizer, "To be or not to be", max_new_tokens=100)
print(text)
```

**Sample Output:**
```
To be or not to be, that in the senses of the bear in the world,
And such a man as you have done with me...
```

### 2. Conversational AI (V2)
```python
from chatbot_v2 import ChatBot

bot = ChatBot()
response = bot.generate_response("How are you today?")
print(response)
```

**Sample Conversation:**
```
ğŸ§‘ You: Hi, how are you today?
ğŸ¤– Assistant: Hello! I'm doing great, thank you for asking. How about you? What's been going on in your day?

ğŸ§‘ You: I'm feeling a bit stressed about work
ğŸ¤– Assistant: I hear you. Work stress can be really overwhelming. What's been weighing on your mind lately? Sometimes just talking about it can help lighten the load.
```

## ğŸ“Š Model Performance

### V1 (Shakespeare Model)
- **Training Data**: ~1M tokens (Shakespeare corpus)
- **Final Loss**: Train: 1.71 | Validation: 1.83
- **Characteristics**: Excellent at archaic language patterns, character names, dialogue structure

### V2 (Chat Model) 
- **Training Data**: ~8.4K tokens (41 conversations)
- **Dataset**: Human-Assistant conversational pairs
- **Capabilities**: Casual conversation, emotional support, question answering

## ğŸ”§ Configuration

Edit `config.py` to customize model architecture:

```python
# Training parameters
batch_size = 32         # Batch size
block_size = 128        # Context window length
max_iters = 8000        # Training iterations
learning_rate = 1e-3    # Learning rate

# Model architecture
embed_dim = 128         # Embedding dimension
num_heads = 8           # Attention heads
num_layers = 4          # Transformer layers
```

## ğŸ¯ Key Components

### 1. **Tokenizer** (`tokenizer.py`)
- Character-level tokenization
- Handles encoding/decoding of text
- Vocabulary built from training data

### 2. **Model** (`model.py`)
- GPT-style transformer architecture
- Self-attention mechanisms
- Feed-forward networks
- Layer normalization

### 3. **Training** (`train.py`)
- AdamW optimizer
- Cross-entropy loss
- Periodic validation
- Model checkpointing

### 4. **Chat Interface** (`chatbot_v2.py`)
- Interactive conversation
- Temperature control
- Response generation
- Context management

## ğŸ“ˆ Training Tips

1. **Start Small**: Begin with smaller models to validate your setup
2. **Monitor Loss**: Watch both training and validation loss curves
3. **Adjust Temperature**: Use 0.8-1.2 for generation (lower = more conservative)
4. **Context Length**: Longer contexts require more memory but better coherence
5. **Dataset Quality**: Clean, diverse data leads to better performance

## ğŸ› Troubleshooting

### Common Issues

1. **CUDA Out of Memory**
   - Reduce `batch_size` in config.py
   - Reduce `block_size` for shorter contexts

2. **Poor Generation Quality**
   - Train for more iterations
   - Increase model size (embed_dim, num_layers)
   - Improve dataset quality

3. **Import Errors**
   - Ensure virtual environment is activated
   - Install requirements: `pip install -r requirements.txt`

## ğŸ”® Future Improvements

- [ ] **Byte-pair encoding** for better tokenization
- [ ] **Beam search** for better generation
- [ ] **Fine-tuning** capabilities
- [ ] **Web interface** for easier interaction
- [ ] **Model quantization** for efficiency
- [ ] **Multi-turn conversation** memory
- [ ] **Reinforcement learning** from human feedback

## ğŸ“ Version History

- **V1.0**: Shakespeare text generation model
- **V2.0**: Conversational AI with enhanced architecture
- **V2.1**: Improved chat interface and dataset

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch: `git checkout -b feature-name`
3. Commit changes: `git commit -am 'Add feature'`
4. Push to branch: `git push origin feature-name`
5. Submit a pull request

## ğŸ“œ License

This project is open source and available under the [MIT License](LICENSE).

## ğŸ™ Acknowledgments

- Inspired by the original GPT architecture
- Built with PyTorch framework
- Character-level tokenization approach
- Educational implementation for learning purposes

---

**â­ Star this repository if you found it helpful!**

For questions or issues, please open an issue in the repository.
